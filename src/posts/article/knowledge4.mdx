---
title: "Beads to Bytes: How the Abacus led to Apple!"
created: 2024-05-11
---
*This is part 4 of the 'Understanding Knowledge Consumption' series where we explore how humans developed and utilised tools for enhancing their memory and information layer. Read [Part 1](https://navinsjourney.com/article/knowledge1), [Part 2](https://navinsjourney.com/article/knowledge2) and [Part 3](https://navinsjourney.com/article/knowledge3)*
<p>&nbsp;</p>
---

A couple of days back I wrote about how early humans used different tools for storing information—cave arts, the wooden tablet, and later writing and how did they use it for accounting purposes—that made me curious..

### If we were already keeping accounting records, then when did we learn mathematics and how did we do it?

Do you remember studying this |||| ? One horizontal line that would cross these, and we'd call it 5, can't recall?

**Tally! our first cognitive artifact**—external objects or tools that help us perform mental tasks and augment our cognitive abilities. They help us store, process and retrieve information!

Early humans used bones and drew tally marks to keep record long before writing was invented.

It is not surprising to know that we invented one of the most complex calculating tools just 800 years (trust me, it is not that much) after we learnt to write - the Abacus, which is still very prominently used in our schools to teach mathematics!

### But it took us another 4300 years to go to the next stage of tool (can you guess?)

During this 4300-year period, the Romans (2nd century BCE), the Chinese (2nd century BCE), and the Japanese (14th century CE) developed their respective versions of the abacus, each incrementally better than the previous versions.

All this development was happening to aid the latest advancements in mathematics. During this time, mathematicians like Euclid (300 BCE) and Diophantus (250 CE) made significant contributions to geometry and algebra.

**Fast forward to the 16th century..**

The early and mid 16th century saw a lot of innovation, the sheer amount of tools being developed hinted towards the next phase of tools.

We had Logarithms (1614), Napier's Bones (1617), Slide Rules (1620s), and then...

1642, Blaise Pascal, while experimenting with geared wheels, to help his father perform complex tax calculations, the Pascaline.

### This marked as one of the most prominent events in our computing history until the next moment..

Inspired by the design of Abacus, Charles Babbage conceptualised the Difference Engine in 1822, and later went on to conceptualise the Analytical Engine, which led him to be called as the father of computer!

The Analytical Engine laid the groundwork for modern computer architecture. It introduced concepts like conditional branching, looping, and memory, which are fundamental to modern computing.

### The Industrial Revolution in the 18th-19th century gave us the computer that we see today..

The invention of the electric motors and other electrical components were crucial to the computing world.

In 1945, we had built the ENIAC, one of the first electronic computing devices. It had around 17,000 vacuum tubes and could perform about 5,000 additions per second. It was fully programmable using plug boards and switches, allowing for more flexibility and easier reprogramming.

This was another early 16th century which was repeating itself, the amount of innovation happening across the globe was again hinting towards something big .. and it was not too late when we saw IBM, MITS (Altair) and Apple build personal computers.

Our relationship with information, data, work, life - literally everything changed, or rather revolutionised after that.

**We'll go deeper into this in the next part, but for now, zoom out for a bit.. and think**

It's not even been 100 years since we invented the ENIAC and we now have smartphones, AI, GPUs and all sorts of tools that are defining the next phase of our life..

The journey from the abacus to Apple is a testament to human ingenuity and the relentless pursuit of progress. As we stand at the precipice of a new era in computing, with artificial intelligence, quantum computing, and the Internet of Things poised to revolutionize our world, it's clear that this story is far from over ..

*Imo, it's the 16th century all over again, and this is the new normal..*